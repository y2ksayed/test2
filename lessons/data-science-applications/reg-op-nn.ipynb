{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Regularization and Optimization for Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "_Authors: Matt Brems and Justin Pounders (but mainly Matt)_\n",
    "\n",
    "The principal topic of the day is **how to control bias/variance tradeoff in neural nets.**\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of the lesson, students should be able to:\n",
    "- Explain how L1/L2, dropout, and early stopping regularization work and implement these methods in Keras\n",
    "- Implement methods for speeding up learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/tiffanykelly/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./cell_phone_churn.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('state', axis=1)\n",
    "\n",
    "# Many ways to binarize:\n",
    "data['intl_plan'] = data['intl_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "data['vmail_plan'] = data['vmail_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "\n",
    "# data['intl_plan'] = data['intl_plan'].map({'yes': 1, 'no': 0})\n",
    "# data['intl_plan'] = (data['intl_plan'] == 'yes').astype('int')\n",
    "# data['intl_plan'] = (data['intl_plan'] == 'yes') * 1\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('churn', axis=1)\n",
    "y = data['churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple neural net to model churn\n",
    "\n",
    "Let's build this:\n",
    "\n",
    "- a dense network,\n",
    "- one input layer,\n",
    "- one hidden layer \n",
    "  - same number of nodes as input layer,\n",
    "  - ReLU activation\n",
    "- single node output (for binary classification)\n",
    "  - sigmoid activation\n",
    "  \n",
    "> **Fun fact**: If we dropped the hidden layer, this model would just be logistic regression!  Can you prove that to yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert model here...\n",
    "n_input = X_train.shape[1]\n",
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(18, input_dim=n_input, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile it\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look, Ma, the machine is learning!\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['acc'][-1], history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without regularization, **val_loss:**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/overkill.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='l1'></a>\n",
    "## Regularization Method 1: L1 and L2 \n",
    "---\n",
    "Just as we did with linear and logistic regression, we can use `L1` and `L2` regularization on our neural networks.\n",
    "\n",
    "Neural networks are just large combinations of linear functions that are modified using some activation function:\n",
    "\n",
    "$$z = b_0 + \\sum_{j=1}^p w_j x_j$$\n",
    "$$a = g(z)$$\n",
    "\n",
    "Where $x_j$ is one input (i.e. one observation's blood pressure, one observation's sex, etc.), $w_j$ is the weight/coefficient for that particular variable, $b_0$ is our bias, and $g$ is our activation function. If we used a sigmoid function as we would for logistic regression, $g$ would be:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "After we've done this for every node, we can then compute the loss for every node as a function of their parameters:\n",
    "$$\\text{loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i)$$\n",
    "\n",
    "This is our average loss. In a regression context, this is usually mean squared error; in a classification context this might be categorical cross-entropy or something else. This would be our loss function *without regularization*.\n",
    "\n",
    "We'll then implement gradient descent:\n",
    "\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{loss}}{\\partial w_j}$$\n",
    "\n",
    "where $\\alpha$ is our learning rate and $\\frac{\\partial \\text{loss}}{\\partial w_j}$ represents the partial derivative of our loss function with respect to our weight $w_j$.\n",
    "\n",
    "This is how we implement gradient descent **without regularization**.\n",
    "\n",
    "#### So, how do we implement gradient descent with `L1` or `L2` regularization?\n",
    "\n",
    "> We just change the loss function to add a penalty! If we want to add a penalty term, we do the **exact same thing** we did with linear or logistic regression:\n",
    "\n",
    "$$\\text{L2 regularized loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||w_{[l]}||^2$$\n",
    "\n",
    "Now, $$\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_{[l]}} = \\frac{\\partial \\text{loss}}{\\partial w_j} + \\frac{\\lambda}{m}w_j$$\n",
    "and\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_j}$$\n",
    "\n",
    "In this example we used `L2` regularization, although `L1` works in the same way. You may see `L2` regularization referred to as \"*weight decay*.\"\n",
    "\n",
    "**Practical Note:** According to Andrew Ng, `L2` (as opposed to `L1`) is generally used for regularizing neural networks and it's rare to find `L1`.\n",
    "\n",
    "As before, $\\lambda$ is a hyperparameter to be selected by constructing multiple networks and identifying which value performs the best.\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, our weights get closer and closer to 0 (just like when we regularized our linear models before).\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, if we're using the `sigmoid` or `tanh` activation functions, we force our weights to stay in that \"linear\" region in the middle. This speeds up the learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_l2 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "model_l2.add(Dense(\n",
    "    18,\n",
    "    input_dim=n_input,\n",
    "    activation='relu',\n",
    "    kernel_regularizer=regularizers.l2(0.01)\n",
    "))\n",
    "model_l2.add(Dense(\n",
    "    1,\n",
    "    activation='sigmoid',\n",
    "    kernel_regularizer=regularizers.l2(0.01)\n",
    "))\n",
    "\n",
    "model_l2.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "history_l2 = model_l2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_l2 = history_l2.history['loss']\n",
    "test_loss_l2 = history_l2.history['val_loss']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.plot(train_loss_l2, label='L2 Training loss', color='darkred')\n",
    "plt.plot(test_loss_l2, label='L2 Testing loss', color='pink')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_l2.history['acc'][-1], history_l2.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `L2` regularization and $\\lambda = 0.01$, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Regularization Documentation](https://keras.io/regularizers/)\n",
    "- [Kernel vs. Activity Regularizers](https://github.com/keras-team/keras/issues/3236)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Implementation in Tensorflow](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/regularizers)\n",
    "- [Example in Tensorflow](http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## Regularization Method 2: Dropout\n",
    "---\n",
    "There's another method of regularizing our terms that is specifically designed for neural networks, called **dropout regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we've constructed a neural network. We've decided on the number of layers we want and the number of nodes in each layer. (We might say that we've decided on the **topology** or **structure** of our network.)\n",
    "\n",
    "![](./assets/original_nn.jpeg)\n",
    "\n",
    "However, a densely connected network like this will almost certainly overfit. Our network is learning a parameter for every single connection.\n",
    "> In the above example, we have 55 parameters being learned - and this is a very simple network, all things considered.\n",
    "\n",
    "> We can overcome this by using **dropout regularization**. \n",
    "\n",
    "In dropout regularization, we randomly **drop** units (nodes) in our neural network ***during our training phase only***. We assign a probability of each node disappearing. Then, we essentially perform a coinflip for every node to turn that node \"on\" or \"off.\"\n",
    "\n",
    "Let's go through an example to illustrate this: For simplicity, we'll say we've assigned a 0.5 probability of keeping to every node in the network above. Then, for every node, we flip a coin, and if the coin lands on heads, the node remains, if it lands on tails, the node disappears. After we've done this for every node, we're left with a new network that looks something like this:\n",
    "\n",
    "![](./assets/after_dropout.jpeg)\n",
    "\n",
    "<!--\n",
    "Image sources: https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\n",
    "Also, it seems, this site: http://cs231n.github.io/neural-networks-2/\n",
    "-->\n",
    "\n",
    "Let's explicitly lay out the general workflow you would follow:\n",
    "\n",
    "1. Specify the **topology** of your neural network.\n",
    "2. Initialize your weights and biases.\n",
    "3. Specify the \"keeping probabilities\" for every node. (Generally, we'll assign the same probability to all nodes in each layer and usually the same probability to all hidden layers.)\n",
    "4. Perform a \"coin flip\" for each node and drop out the chosen nodes.\n",
    "5. Run through one epoch of training.\n",
    "6. Repeat steps 4 and 5 for each epoch of training.\n",
    "\n",
    "**Check:** If I drop out a node during one of my epochs, does it disappear from my final network?\n",
    "\n",
    "#### So, what does this do?\n",
    "<!-- <br/> -->\n",
    "The intuition behind dropout is that, since each node has a probability of disappearing at any time, the neural network is disincentivized from allocating too much power to any one weight. It has a similar effect as imposing an L2 penalty: the magnitude of our weights shrinks.\n",
    "\n",
    "**Check:** What might be some potential problems with doing this?\n",
    "\n",
    "<!--\n",
    "expected values of nodes changes; induces bias\n",
    "-->\n",
    "\n",
    "#### Inverted Dropout\n",
    "\n",
    "In order to avoid any issues with the expected values of our nodes changing, we adjust our results accordingly by a method called **inverted dropout**.\n",
    "\n",
    "If we have a hidden layer with 100 nodes and each node has a 80% probability of being \"staying turned on,\" we only have 80% of those inputs to our node. As a result, we expect that the combined input to our node $z = b_0 + \\sum_{i=1}^pw_ix_i$ will be off by about 20%. (Those interested in probability and research might note that the Binomial distribution is a very convenient model for neural networks and dropout.)\n",
    "\n",
    "When using inverted dropout, we adjust $z$ by the \"keeping probability.\"\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_{original} &=& b_0 + \\sum_{i=1}^pw_ix_i \\\\\n",
    "\\Rightarrow z_{dropout} &=& b_0 + \\sum_{i\\in\\{included\\_nodes\\}}w_ix_i \\\\\n",
    "\\Rightarrow z_{inverted\\_dropout} &:=& z_{dropout} / 0.8 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "#### Test time:\n",
    "\n",
    "So we've now run through every epoch of our training phase and we're ready to apply our neural network to our validation or testing data. Are we going to apply dropout to this data as well?\n",
    "\n",
    "**NO.**\n",
    "\n",
    "#### Best practices:\n",
    "\n",
    "- Don't set any keeping probabilities for layers you where you don't want to drop any nodes. (What might be examples of these layers?)\n",
    "<!--\n",
    "Input and output layers\n",
    "-->\n",
    "- You'll generally want to specify a single keeping probability on all the layers on which you want to apply dropout, instead of specifying different keeping probabilities for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model_dropout = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_dropout.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_dropout.add(Dropout(0.5)) # refers to nodes in the first hidden layer\n",
    "model_dropout.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_dropout = model_dropout.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test), \n",
    "    epochs=100,\n",
    "    batch_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_dropout = history_dropout.history['loss']\n",
    "test_loss_dropout = history_dropout.history['val_loss']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.plot(train_loss_dropout, label='Dropout Training loss', color='darkgreen')\n",
    "plt.plot(test_loss_dropout, label='Dropout Testing loss', color='lightgreen')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dropout.history['acc'][-1], history_dropout.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Dropout, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Dropout Documentation](https://keras.io/layers/core/#dropout)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "- [List of examples in Tensorflow](https://programtalk.com/python-examples/tensorflow.nn.dropout/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopping'></a>\n",
    "## Regularization Method 3: Early Stopping\n",
    "---\n",
    "The third method of regularization that we'll discuss today is called early stopping.\n",
    "</br>\n",
    "If we run though all our epochs of training and plot both our training and validation error, we'll typically see something like this:\n",
    "\n",
    "![](./assets/train-val-error-reduced.png)\n",
    "*source: [Prechelt, 1997](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) *\n",
    "\n",
    "**Check:** What is happening in this plot?\n",
    "\n",
    "Early stopping does exactly what its name implies: it stop the training process early. Instead of continuing training through every epoch, once the validation error begins to increase, our algorithm stops because it has (in theory) found the minimum for the validation loss.\n",
    "\n",
    "This might seem like a simple and robust solution to overfitting, but it can run into problems.\n",
    "\n",
    "<details>\n",
    "![](./assets/validation-error-real.png)\n",
    "</details>\n",
    "\n",
    "There is debate over how often this problem occurs. You can generally plot both the training and validation loss, see if you're getting multiple optima. If you are, there are multiple suggested techniques to combat this problem in the [paper reference above](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_es = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "model_es.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_es.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_es.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "history_es = model_es.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=None,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_es = history_es.history['loss']\n",
    "test_loss_es = history_es.history['val_loss']\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='violet')\n",
    "plt.plot(test_loss, label='Testing loss', color='lavender')\n",
    "plt.plot(train_loss_es, label='ES Training loss', color='navy')\n",
    "plt.plot(test_loss_es, label='ES Testing loss', color='skyblue')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_es.history['acc'][-1], history_es.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With early stopping, **val_loss:**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras EarlyStopping Documentation](https://keras.io/callbacks/#earlystopping)\n",
    "- [Keras EarlyStopping Example](http://parneetk.github.io/blog/neural-networks-in-keras/)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow.Keras.callbacks.EarlyStopping Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "Today, we learned about three different methods of regularizing our neural networks: `L2` regularization, dropout, and early stopping.\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "As promised, managing bias and variance takes a lot of our attention. If our bias or variance are high, it's likely that our model isn't performing as well as it could.\n",
    "\n",
    "A workflow for how you should address this (in the context of neural networks and beyond) is as follows:\n",
    "\n",
    "- Do we have high bias? (i.e. are we performing poorly on our training set?)\n",
    "    - If so:\n",
    "        - let's build a more complex model / bigger network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's train longer!\n",
    "- Do we have high variance? (i.e. are we performing poorly on our test/holdout set?)\n",
    "    - If so:\n",
    "        - let's gather more data!\n",
    "            - Usually very difficult, but we should use \"data augmentation\" if we can!\n",
    "        - let's build a simpler model / smaller network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's regularize!\n",
    "    - Once we're satisfied, return to the bias question and repeat.\n",
    "    \n",
    "**Note:** Before deep learning, most tools for handling high bias or high variance adversely affected the other. However, depending on the amount of data we have and how complex our network is, it's often the case that we can drastically reduce variance with out affecting bias.\n",
    "\n",
    "<a id='references'></a>\n",
    "## References and Resources:\n",
    "\n",
    "- [DeepLearning.ai](https://www.deeplearning.ai/), Andrew Ng's Coursera course on Deep Learning\n",
    "  - The videos from this course are on a [YouTube Channel](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/featured)   \n",
    "<br>\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/), textbook written by Ian Goodfellow, creator of Generative Adversarial Networks (GANs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
